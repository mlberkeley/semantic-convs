{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import matplotlib.font_manager\n",
    "import res_utils as ru\n",
    "from scipy.ndimage.interpolation import shift\n",
    "from PIL import ImageFont\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from scae.util import vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# plt.rcParams.update({'font.size': 14})\n",
    "plt.rcParams.update({'text.usetex': False})\n",
    "# matplotlib.rcParams['text.latex.preamble'] = [\n",
    "#     r'\\usepackage{amsmath}',\n",
    "#     r'\\usepackage{amssymb}']\n",
    "# plt.rcParams.update({'font.family': 'serif'})\n",
    "# plt.rcParams.update({'font.family': 'serif', 'font.serif':['Computer Modern']})\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to see the fonts on your system and choose a different font\n",
    "# matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_range(v):\n",
    "    return (v-v.min())/(v.max()-v.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization of shape, color and location\n",
    "\n",
    "In this notebook, we are going to set up a simple scene analysis problem that can be solved with the resonator network. This example generates a scene by combining several factors to create an object: the object is a conjunction of shape, color and location. The shapes of the objects are given by fixed templates (letters chosen from a font). The goal will be to use VSA principles and resonator networks to infer the factors of each object from the scene.\n",
    "\n",
    "First, lets get some letters for the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will determine the size of the scene\n",
    "patch_size=[56, 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 3, 56, 56])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#font = ImageFont.load_default().font\n",
    "#font = ImageFont.load_path(u'/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf').font\n",
    "#font = ImageFont.truetype( u'/usr/share/fonts/truetype/ttf-dejavu/DejaVuSans.ttf', size=18)\n",
    "font =  ImageFont.truetype(u'Ubuntu-Title.ttf', size=18)\n",
    "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "font_ims = []\n",
    "fim_size = (patch_size[0],patch_size[1],3)\n",
    "\n",
    "for l in letters:\n",
    "\n",
    "    font_obj = font.getmask(l)\n",
    "    \n",
    "    imtext = np.array(font_obj)\n",
    "    imsize = font_obj.size #font.getsize(l)\n",
    "\n",
    "    imtext = np.tile(imtext.reshape((imsize[1], imsize[0], 1)), (1,1,3))\n",
    "    imtext = imtext[:patch_size[0], :patch_size[1], :]\n",
    "    \n",
    "    imsize = imtext.shape\n",
    "    \n",
    "    fim = np.zeros(fim_size)\n",
    "    \n",
    "    fimr = int(np.floor((fim_size[0] - imsize[0])/2))\n",
    "    fimc = int(np.floor((fim_size[1] - imsize[1])/2))\n",
    "    \n",
    "    fim[fimr:(fimr+imsize[0]), fimc:(fimc+imsize[1]), :] = imtext/255\n",
    "    \n",
    "    font_ims.append(fim)\n",
    "\n",
    "t_font_ims = torch.Tensor(font_ims).permute(0, 3, 1, 2)\n",
    "vis.plot_image_tensor(t_font_ims)\n",
    "t_font_ims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding locations with the exponentiation trick\n",
    "\n",
    "The first step of the scene analysis is to encode the scene into a high-dimensional VSA vector. This VSA encoding has some special properties. The first is that the encoding vectors for the pixels are designed in a special way such that the properties of translation are available as a simple operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating the scene in this fashion is possible because the pixels are encoded using the \"exponentiation trick\", where each pixel position is encoded by a single base vector raised to a power. For example, the vertical position 10 is encoded as $V^{10}$. Similarly, the horizontal position has base vector and is also exponentiated by the pixel location. The 2-D location is then indexed as the binding between the vector encodings, e.g. the position 10, 20 is given by $V^{10} \\odot H^{20}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(3e4)\n",
    "\n",
    "# These are special base vectors for position that loop\n",
    "Vt = ru.cvec(N, font_ims[0].shape[0])\n",
    "Ht = ru.cvec(N, font_ims[0].shape[1])\n",
    "\n",
    "# This is a set of 3 independently random complex phasor vectors for color\n",
    "Cv = ru.crvec(N, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pix(im, Vt, Ht):\n",
    "        N = Vt.shape[0]\n",
    "\n",
    "        image_vec = 0.0 * ru.cvec(N, 1)\n",
    "\n",
    "        for m in range(im.shape[0]):\n",
    "            for n in range(im.shape[1]):\n",
    "                P_vec = Vt[m] * Ht[m]\n",
    "                image_vec += P_vec * im[m, n]\n",
    "\n",
    "        return image_vec\n",
    "\n",
    "def encode_pix_rgb(im, Vt, Ht, Cv):\n",
    "    N = Vt.shape[0]\n",
    "\n",
    "    image_vec = torch.as_tensor(0.0 * ru.cvec(N, 1), dtype=torch.cfloat).cuda()\n",
    "    for m in range(im.shape[0]):\n",
    "        Vt_m__Ht_n = Vt ** m * Ht ** 0\n",
    "        for n in range(im.shape[1]):\n",
    "            for c in range(im.shape[2]):\n",
    "                P_vec = Cv[c] * Vt_m__Ht_n\n",
    "                image_vec += P_vec * im[m, n, c]\n",
    "            Vt_m__Ht_n *= Ht\n",
    "\n",
    "\n",
    "    return image_vec\n",
    "\n",
    "def decode_pix(image_vec, Vt, Ht):\n",
    "    N = Vt.shape[0]\n",
    "    im_r = np.zeros(fim_size)\n",
    "\n",
    "    for m in range(im_r.shape[0]):\n",
    "        for n in range(im_r.shape[1]):\n",
    "            P_vec = (Vt ** m) * (Ht ** n)\n",
    "            im_r[m, n] = np.real(np.dot(np.conj(P_vec), image_vec)/N)\n",
    "    return np.clip(im_r, 0, 1)\n",
    "\n",
    "def decode_pix_rgb(image_vec, Vt, Ht, Cv):\n",
    "    N = Vt.shape[0]\n",
    "    im_r = torch.zeros(fim_size).cuda()\n",
    "\n",
    "    for m in range(im_r.shape[0]):\n",
    "        Vt_m__Ht_n = Vt ** m * Ht ** 0\n",
    "        for n in range(im_r.shape[1]):\n",
    "            for c in range(im_r.shape[2]):\n",
    "                P_vec = Cv[c] * Vt_m__Ht_n\n",
    "                im_r[m, n, c] = torch.real(torch.dot(torch.conj(P_vec), image_vec)/N)\n",
    "            Vt_m__Ht_n *= Ht\n",
    "    return torch.clip(im_r, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSA:\n",
    "    def __init__(self, Vt, Ht, Cv, imshape=patch_size, device=\"cuda\"):\n",
    "        self.Vt = torch.as_tensor(Vt, dtype=torch.cfloat).to(device)\n",
    "        self.Ht = torch.as_tensor(Ht, dtype=torch.cfloat).to(device)\n",
    "        self.Cv = torch.as_tensor(Cv, dtype=torch.cfloat).to(device)\n",
    "        \n",
    "        # VSA vec length\n",
    "        self.V = self.Cv.shape[-1]\n",
    "        Hr = torch.range(0, imshape[0]-1).to(device)[:, None, None, None].expand(imshape[0], 1, 1, self.V)\n",
    "        Vr = torch.range(0, imshape[1]-1).to(device)[None, :, None, None].expand(1, imshape[1], 1, self.V)\n",
    "        \n",
    "        self.P_vec = self.Ht[None, None, None, :].expand(1, imshape[1], 1, self.V).pow(Hr) \\\n",
    "                    * self.Vt[None, None, None, :].expand(imshape[0], 1, 1, self.V).pow(Vr) \\\n",
    "                    * self.Cv[None, None, :, :]\n",
    "        \n",
    "        # Position \"hashing\" vectors (Verticals, Horizontals, Channels, VSA vec lengths)\n",
    "        print(f\"P_vec shape: {self.P_vec.shape}\")\n",
    "        self.device = device\n",
    "        \n",
    "    def encode_pix(self, im):\n",
    "        # Weighted sum of all P_vectors over pixels and channels (superposition of pixel values, embedded with P_vec)\n",
    "        return (self.P_vec.T * im.T).reshape(self.V, -1).sum(-1).T\n",
    "\n",
    "    def decode_pix(self, image_vec):\n",
    "        # Selects pixel value by dotting VSA vec with conjugate of position encoding, and taking real component (clip for stability)\n",
    "        # Old elementwise formula: return_img[m, n, c] = torch.real(torch.dot(torch.conj(self.P_vec[m, n, c]), image_vec) / self.R)\n",
    "        return torch.clip(torch.real(torch.conj(self.P_vec) * image_vec[None, None, None, :]).sum(-1) / self.V, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 56, 3) (56, 30000) (56, 30000) torch.Size([3, 30000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axquaris/.conda/envs/crystalize/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  if __name__ == '__main__':\n",
      "/home/axquaris/.conda/envs/crystalize/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VSA' object has no attribute 'R'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-78-bb4d8738b5ad>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfont_ims\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mVt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mHt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mvsa\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mVSA\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mVt\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mHt\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'cuda'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mtotal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfont_ims\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-77-b8a6b721780f>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, Vt, Ht, Cv, imshape, device)\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mP_vec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mHt\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mHr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m                     \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mVt\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mVr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m                     \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCv\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[0;31m# Position \"hashing\" vectors (Verticals, Horizontals, Channels, VSA vec lengths)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'VSA' object has no attribute 'R'"
     ]
    }
   ],
   "source": [
    "print(font_ims[0].shape, Vt.shape, Ht.shape, Cv.shape)\n",
    "with torch.no_grad():\n",
    "    vsa = VSA(Vt[0], Ht[0], Cv, device='cuda')\n",
    "    \n",
    "    total = len(font_ims)\n",
    "    total_encode = 0.\n",
    "    total_decode = 0.\n",
    "    ims = []\n",
    "    reps = []\n",
    "    recs = []\n",
    "    \n",
    "    for i, im in enumerate(font_ims):\n",
    "        input_img = torch.as_tensor(im, dtype=torch.float).cuda()\n",
    "        ims.append(input_img.cuda())\n",
    "\n",
    "        tst = time.time()\n",
    "        f_vec = vsa.encode_pix(input_img)\n",
    "        total_encode += time.time() - tst\n",
    "        reps.append(f_vec)\n",
    "        \n",
    "        # translate the image\n",
    "        f_vec_tr = f_vec * vsa.Vt**i * vsa.Ht**i\n",
    "\n",
    "        tst = time.time()\n",
    "        f_im = vsa.decode_pix(f_vec_tr)\n",
    "        total_decode += time.time() - tst\n",
    "        recs.append(f_im.cpu())\n",
    "    \n",
    "    f_vec_composite = reps[0] * vsa.Vt**-15 + reps[1] + reps[2] * vsa.Ht**15 + reps[3] * vsa.Vt**15 * vsa.Ht**15\n",
    "    tst = time.time()\n",
    "    f_im = vsa.decode_pix(f_vec_composite)\n",
    "    print(f\"composite decode time: {time.time() - tst:.3}s\") \n",
    "    plt.imshow(f_im.cpu())\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"avg encode time: {total_encode/total:.3}s\")\n",
    "    print(f\"avg decode time: {total_decode/total:.3}s\")\n",
    "    vis.plot_image_tensor(torch.stack(ims).permute(0, 3, 1, 2))\n",
    "    vis.plot_image_tensor(torch.stack(recs).permute(0, 3, 1, 2))\n",
    "#     plt.show()\n",
    "    \n",
    "    del vsa\n",
    "    torch.cuda.empty_cache()\n",
    "#     print(torch.cuda.memory_stats(device=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toroidal image encoding\n",
    "\n",
    "There is a second special property to these vectors, which enables treating the image like a torus -- as we translate to the right and move off of the right side of the image, we will end up on the left side. Same for top and bottom. For this to be possible, we design the base vectors by considering the identity vector and the roots of the identity vector.\n",
    "\n",
    "What is the identity vector? Since the operation we are considering is element-wise multiply, the identity vector is simply a vector where each entry is 1. \n",
    "\n",
    "When we apply the exponentiation trick to a root of the identity vector, then as we increase the exponent, we will eventually go in a loop. We choose the order of the root based on how long we want the loop to be. If we choose a square root of the identity, then we will loop every two steps. If we choose the 4th root, we will loop every 4 steps. Here, we chose a loop the size of the image -- 56 pixels.\n",
    "\n",
    "What are the roots of the identity vector? Of course, the square roots of 1 are (+1, -1). The 4th roots of 1 are (1, i, -1, -i). The N-th root of 1 are the $N$ points around the complex plane: $e^{2 \\pi i k / N} \\forall k = 1, ..., N$. Thus, to form a VSA vector that will loop when raised to the $N$-th power, we choose one of the $N$ roots of identity randomly for each entry of the vector. This is implemented by the function $\\verb|cvecl|$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_size = 10\n",
    "loop_vec = ru.cvec(N, loop_size)[0]\n",
    "\n",
    "pows = np.linspace(-15, 15, 200)\n",
    "sims = np.zeros_like(pows)\n",
    "\n",
    "for i, p in enumerate(pows):\n",
    "    sims[i] = np.real(np.dot(np.conj(loop_vec).T, loop_vec ** p)) / N\n",
    "                      \n",
    "plt.plot(pows, sims)\n",
    "plt.xlabel('Exponent')\n",
    "plt.ylabel('Similarity')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are examining the similarity between a loop vector and its powers. What we see is high similarity at power 1, as well as at powers 11, and -9. Because we designed the vector based on the roots, we can control how long it takes for it to loop!\n",
    "\n",
    "Note here we are also exponentiating the vectors by fractional powers -- we have the ability to perform fractional binding in the complex domain! This means that as we incremenet the exponent slightly we move smoothly along a manifold in the high-dimensional vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_size = 1000\n",
    "loop_vec = ru.cvec(N, loop_size)[0]\n",
    "\n",
    "pows = np.linspace(-15, 15, 200)\n",
    "sims = np.zeros_like(pows)\n",
    "\n",
    "for i, p in enumerate(pows):\n",
    "    sims[i] = np.real(np.dot(np.conj(loop_vec).T, loop_vec ** p)) / N\n",
    "                      \n",
    "plt.plot(pows, sims)\n",
    "\n",
    "# compare to the sinc function\n",
    "plt.plot(pows+1, np.sin(np.pi*pows)/(np.pi*pows))\n",
    "\n",
    "plt.xlabel('Exponent')\n",
    "plt.ylabel('Similarity')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose a very large loop size (or equivalently a completely random complex VSA vector), then we see a very special curve appear -- the sinc function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with correlations\n",
    "\n",
    "Now that we have a way to encode position (that is smooth and can loop), we next need to consider how to deal with encoding of shape and color. \n",
    "\n",
    "The representation for the shape of the object will be linked closely to the representation of pixel location. In essence, the shape of the object is the set of pixel locations that are active. Thus the representation of a particular letter will be the sum of the vector encoding of each pixel location. \n",
    "\n",
    "However, the issue that arises is that the shapes of the different objects are actually correlated. Conceptually, we want each letter to act as a symbol, meaning that it is orthogonal to the other letters. But the objects share many features in the pixel space. These correlations in the pixel space will cause issues with the resonator network when trying to solve the factorization problem. The solution to this issue is to use whitening on the patterns. Whitening orthogonalizes the patterns by adjusting the pixel magnitudes based on how many pixels are shared across letters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_whiten(X):\n",
    "\n",
    "    U, s, Vh = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "    # U and Vt are the singular matrices, and s contains the singular values.\n",
    "    # Since the rows of both U and Vt are orthonormal vectors, then U * Vt\n",
    "    # will be white\n",
    "    X_white = np.dot(U, Vh)\n",
    "\n",
    "    return X_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_im_vecs = np.zeros((len(font_ims), np.prod(font_ims[0].shape[:2])))\n",
    "\n",
    "for i in range(len(font_ims)):\n",
    "    font_im_vecs[i, :] = font_ims[i].mean(axis=2).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "for i in range(26):\n",
    "    plt.subplot(3, 9, i+1)\n",
    "    plt.imshow(font_im_vecs[i].reshape(font_ims[0].shape[:2]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_ims_w = svd_whiten(font_im_vecs.T).T\n",
    "plt.figure(figsize=(20,6))\n",
    "for i in range(26):\n",
    "    plt.subplot(3, 9, i+1)\n",
    "    plt.imshow(font_ims_w[i].reshape(font_ims[0].shape[:2]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whitened shapes look similar to the original shape with certain pixel emphasized or de-emphasized. We can see now that the shapes are orthogonalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_xcorr = np.dot(font_im_vecs, font_im_vecs.T)\n",
    "whiten_xcorr = np.dot(font_ims_w, font_ims_w.T)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(original_xcorr)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(whiten_xcorr)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will just encode the templates and the decorrelated templates into a vector. Note that we are not considering the colors and just using the pixel locations for the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_vecs = ru.crvec(N, len(font_ims))\n",
    "tst = time.time()\n",
    "for i in range(len(font_ims)):\n",
    "    print(i, end=\" \")\n",
    "    font_vecs[i,:] = encode_pix(font_ims[i].mean(axis=2), Vt[0], Ht[0])\n",
    "print(\"elapsed:\", time.time() - tst)\n",
    "# this is stupidly slow, can be implemented by just a matrix multiply \n",
    "# instead of a loop for better speed (need to store big matrix in memory though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_vecs_w = ru.crvec(N, len(font_ims))\n",
    "tst = time.time()\n",
    "for i in range(len(font_ims)):\n",
    "    print(i, end=\" \")\n",
    "    font_vecs_w[i,:] = encode_pix(font_ims_w[i].reshape(font_ims[0].shape[:2]), Vt[0], Ht[0])\n",
    "print(\"elapsed:\", time.time() - tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion to the letters, we will also consider color as a factor. Further, we will use 7 different colors. But hmm... 7 colors can't all be orthogonal because color is only a 3 dimensional space! \n",
    "\n",
    "The whitening still has an effect, and is still necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict = {'red': [1, 0, 0], 'green': [0, 1, 0], 'blue': [0, 0, 1], 'cyan': [0, 1, 1],\n",
    "          'magenta':[1,0,1], 'yellow': [1, 1, 0], 'white': [1, 1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_arr = np.array(list(colors_dict.values()))\n",
    "colors_lab = np.array(list(colors_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_im = np.tile(colors_arr.reshape([1, colors_arr.shape[0], colors_arr.shape[1]]), [10, 1, 1])\n",
    "plt.imshow(cols_im*255, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_svd = svd_whiten(colors_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_svd_im = np.tile(colors_svd.reshape([1, colors_svd.shape[0], colors_svd.shape[1]]), [10, 1, 1])\n",
    "plt.imshow(norm_range(cols_svd_im), interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_vecs = np.dot(colors_arr, Cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_vecs_w = np.dot(colors_svd, Cv)\n",
    "color_vecs_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the resonator network\n",
    "\n",
    "Now that we have VSA encodings that allow for translation and that deal with correlations in the templates, we can set-up the scene anlaysis problem as a factorization problem that can be solved by the resonator network. \n",
    "\n",
    "The idea is that the scene is composed of several objects, and each object is composed from several factors -- shape, color, horizontal and vertical location. We can store the atomic vectors for location and the decorrelated vectors for shape and color into the clean-up memories of a resonator network. When we present a scene (encoded into the VSA vector) to the resonator network, it will search through combinations of factors that best matches with the input scene. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_xlabels = [colors_lab, np.array(list(letters)), np.arange(patch_size[0]), np.arange(patch_size[1])]\n",
    "res_xticks = [np.arange(0, len(res_xlabels[0]), 3, dtype='int'),\n",
    "              np.arange(0, len(res_xlabels[1]), 8, dtype='int'),\n",
    "              np.arange(0, len(res_xlabels[2]), 10, dtype='int'),\n",
    "              np.arange(0, len(res_xlabels[3]), 10, dtype='int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are creating the clean-up memories for the resonator network\n",
    "Vspan = font_ims[0].shape[0]\n",
    "Hspan = font_ims[0].shape[1]\n",
    "\n",
    "Vt_span = ru.crvec(N, Vspan)\n",
    "Ht_span = ru.crvec(N, Hspan)\n",
    "\n",
    "for i in range(Vspan):\n",
    "    ttV = i - Vspan//2\n",
    "    \n",
    "    Vt_span[i,:] = Vt[0] ** ttV\n",
    "\n",
    "    \n",
    "for i in range(Hspan):\n",
    "    ttH = i - Hspan//2\n",
    "    \n",
    "    Ht_span[i,:] = Ht[0] ** ttH\n",
    "\n",
    "res_vecs = []\n",
    "\n",
    "res_vecs.append(color_vecs)\n",
    "res_vecs.append(font_vecs)\n",
    "res_vecs.append(Vt_span)\n",
    "res_vecs.append(Ht_span)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scene of three objects with random factors\n",
    "\n",
    "im_idx1 =  np.random.randint(len(font_ims))\n",
    "tH = patch_size[1] * np.random.rand(1)\n",
    "tV = patch_size[0] * np.random.rand(1)\n",
    "rC = np.random.randint(colors_arr.shape[0])\n",
    "print(rC, colors_lab[rC])\n",
    "\n",
    "t_im1 = font_ims[im_idx1].copy()\n",
    "\n",
    "for i in range(t_im1.shape[2]):\n",
    "    t_im1[:,:,i] = colors_arr[rC, i] * t_im1[:,:,i]\n",
    "\n",
    "t_im1 = shift(t_im1, (tV, tH, 0), mode='wrap', order=1)\n",
    "\n",
    "######\n",
    "im_idx2 = np.random.randint(len(font_ims))\n",
    "tH2 = patch_size[1] * np.random.rand(1)\n",
    "tV2 = patch_size[0] * np.random.rand(1)\n",
    "rC2 = np.random.randint(colors_arr.shape[0])\n",
    "print(rC2, colors_lab[rC2])\n",
    "\n",
    "t_im2 = font_ims[im_idx2].copy()\n",
    "\n",
    "for i in range(t_im2.shape[2]):\n",
    "    t_im2[:,:,i] = colors_arr[rC2, i] * t_im2[:,:,i]\n",
    "\n",
    "t_im2 = shift(t_im2, (tV2, tH2, 0), mode='wrap', order=1)\n",
    "\n",
    "######\n",
    "im_idx3 =  np.random.randint(len(font_ims))\n",
    "tH3 = patch_size[1] * np.random.rand(1)\n",
    "tV3 = patch_size[0] * np.random.rand(1)\n",
    "rC3 = np.random.randint(colors_arr.shape[0])\n",
    "print(rC3, colors_lab[rC3])\n",
    "\n",
    "t_im3 = font_ims[im_idx3].copy()\n",
    "\n",
    "for i in range(t_im2.shape[2]):\n",
    "    t_im3[:,:,i] = colors_arr[rC3, i] * t_im3[:,:,i]\n",
    "\n",
    "t_im3 = shift(t_im3, (tV3, tH3, 0), mode='wrap', order=1)\n",
    "\n",
    "\n",
    "#####\n",
    "t_im = np.clip(t_im1, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(t_im, interpolation='none')\n",
    "plt.tight_layout()\n",
    "    \n",
    "fname = ('figures/smooth_hdmsc-im' \n",
    "        + '-l=' +str(im_idx1) +'-c=' + colors_lab[rC] + '-tV=' + str(tV) + '-tH=' + str(tH)\n",
    "        + '-n' + time.strftime('%y%m%d'))\n",
    "\n",
    "print(fname)\n",
    "# plt.savefig(fname + '.png', format='png')\n",
    "# plt.savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the scene into a VSA vector\n",
    "bound_vec = encode_pix_rgb(t_im, Vt[0], Ht[0], Cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the resonator dynamics\n",
    "tst= time.time()\n",
    "res_hist, nsteps = ru.res_decode_abs(bound_vec, res_vecs, 200)\n",
    "print(\"elapsed\", time.time()-tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the convergence dynamics\n",
    "plt.figure(figsize=(8,3))\n",
    "\n",
    "ru.resplot_im(res_hist, nsteps, labels=res_xlabels, ticks=res_xticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "    \n",
    "fname = ('figures/smooth_hdmsc-res' \n",
    "        + '-l=' +str(im_idx1) +'-c=' + colors_lab[rC] + '-tV=' + str(tV) + '-tH=' + str(tH)\n",
    "        + '-n' + time.strftime('%y%m%d'))\n",
    "\n",
    "print(fname)\n",
    "# plt.savefig(fname + '.png', format='png')\n",
    "# plt.savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the resonator dynamics as it tries to find a solution. Each plot describes one of the 4 factors. The network will hone in on one particular object and find its factorization. The colors show the output of the network, with yellow indicating strong confidence on the output. The network will at first jump around the state space quite chaotically, until a good solution is stumbled upon and as if in a moment of insight the network rapdily converges to a factorization of the scene. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining away to handle multiple objects\n",
    "\n",
    "The resonator network solves the factorization problem for one object at a time. In order to evaluate the rest of the scene, we expalin-away the output of the resonator network and reset the system. Now the resonator network will hone in and factorize a different object, and the process can be repeated for each object. This procedure is analogous to deflation in tensor decompositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_w, out_c = ru.get_output_conv(res_hist, nsteps)\n",
    "print(out_w, out_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we explain away, we want to subtract out the original templates, not the whitened templates\n",
    "res_out = color_vecs[out_w[0]] * font_vecs[out_w[1]] * res_vecs[2][out_w[2]] * res_vecs[3][out_w[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_out_sim = np.real(np.dot(np.conj(res_out)/np.linalg.norm(res_out), bound_vec/np.linalg.norm(bound_vec)))\n",
    "print(res_out_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_vec2 = bound_vec/np.linalg.norm(bound_vec) - res_out_sim * res_out / np.linalg.norm(res_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst= time.time()\n",
    "res_hist2, nsteps2 = ru.res_decode_abs(bound_vec2, res_vecs, 200)\n",
    "print(\"elapsed\", time.time()-tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "\n",
    "ru.resplot_im(res_hist2, nsteps2, labels=res_xlabels, ticks=res_xticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "    \n",
    "fname = ('figures/smooth_hdmsc-res' \n",
    "        + '-l=' +str(im_idx1) +'-c=' + colors_lab[rC] + '-tV=' + str(tV) + '-tH=' + str(tH)\n",
    "        + '-n' + time.strftime('%y%m%d'))\n",
    "\n",
    "print(fname)\n",
    "# savefig(fname + '.png', format='png')\n",
    "# savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_w2, out_c2 = ru.get_output_conv(res_hist2, nsteps2)\n",
    "print(out_w2, out_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_out2 = color_vecs[out_w2[0]] * font_vecs[out_w2[1]] * res_vecs[2][out_w2[2]] * res_vecs[3][out_w2[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_out_sim2 = np.real(np.dot(np.conj(res_out2)/np.linalg.norm(res_out2), bound_vec2/np.linalg.norm(bound_vec2)))\n",
    "print(res_out_sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_vec3 = bound_vec2/np.linalg.norm(bound_vec2) - res_out_sim2 * res_out2 / np.linalg.norm(res_out2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst= time.time()\n",
    "res_hist3, nsteps3 = ru.res_decode_abs(bound_vec3, res_vecs, 200)\n",
    "print(\"elapsed\", time.time()-tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "\n",
    "ru.resplot_im(res_hist3, nsteps3, labels=res_xlabels, ticks=res_xticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "    \n",
    "fname = ('figures/smooth_hdmsc-res' \n",
    "        + '-l=' +str(im_idx1) +'-c=' + colors_lab[rC] + '-tV=' + str(tV) + '-tH=' + str(tH)\n",
    "        + '-n' + time.strftime('%y%m%d'))\n",
    "\n",
    "print(fname)\n",
    "# savefig(fname + '.png', format='png')\n",
    "# savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}